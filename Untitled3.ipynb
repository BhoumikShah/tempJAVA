{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPD+b/asufrpjVHzHWdm3Ud",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhoumikShah/tempJAVA/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "qHACh7hchyAI",
        "outputId": "14685956-5d96-44b0-e5e3-10e397726696"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "HTTP Error 404: Not Found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-14456270304d>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'user-agent'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'my-app'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from newspaper import Article\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "URL = \"\"\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.80 Safari/537.36',\n",
        "    'Content-Type': 'text/html',\n",
        "}\n",
        "\n",
        "max_count = 10  # max 10 news articles per day\n",
        "fmt = '%m/%d/%Y'\n",
        "news_cols = ['index', 'date', 'status_code', 'url', 'news_1_url', 'news_1_text',\n",
        "             'news_1_publish_date', 'news_2_url', 'news_2_text', 'news_2_publish_date',\n",
        "             'news_3_url', 'news_3_text', 'news_3_publish_date', 'news_4_url',\n",
        "             'news_4_text', 'news_4_publish_date', 'news_5_url', 'news_5_text',\n",
        "             'news_5_publish_date', 'news_6_url', 'news_6_text', 'news_6_publish_date',\n",
        "             'news_7_url', 'news_7_text', 'news_7_publish_date', 'news_8_url',\n",
        "             'news_8_text', 'news_8_publish_date', 'news_9_url', 'news_9_text',\n",
        "             'news_9_publish_date']\n",
        "\n",
        "\n",
        "def run_google_news_scrapper(**params):\n",
        "    output_file_name = ''\n",
        "    for key, value in params.items():\n",
        "        if key == 'min_date':\n",
        "            min_date = value\n",
        "        if key == 'output_file':\n",
        "            output_file_name = value\n",
        "\n",
        "    news_data_dict = dict()\n",
        "    columns = []\n",
        "    news_data_dict['date'] = min_date\n",
        "    columns.append('date')\n",
        "    response = requests.get(URL.format(**params), headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    news_data_dict['status_code'] = response.status_code\n",
        "    columns.append('status_code')\n",
        "    if response.status_code != 200:\n",
        "        print(\"******** fail ********** \")\n",
        "        return\n",
        "    # print(response.url)\n",
        "    news_data_dict['url'] = response.url\n",
        "    columns.append('url')\n",
        "    count = 1\n",
        "    for link in soup.find_all('a'):\n",
        "        link_str = str(link.get('href'))\n",
        "        try:\n",
        "            if link_str.startswith(\"https://\") and link_str.find('google.com') == -1 and link_str.find(\n",
        "                    \"https://www.youtube.com/\") == -1 and link_str.find(\"https://www.blogger.com/\") == -1:\n",
        "                article = Article(link_str)\n",
        "                article.download()\n",
        "                article.parse()\n",
        "                # print(link_str)\n",
        "                # print(article.authors)\n",
        "                # print(article.publish_date)\n",
        "                # print(article.text)\n",
        "                news_count = 'news_' + str(count)\n",
        "                news_data_dict[news_count + '_url'] = link_str\n",
        "                news_data_dict[news_count + '_text'] = article.text\n",
        "                news_data_dict[news_count + '_publish_date'] = article.publish_date\n",
        "\n",
        "                columns.append(news_count + '_url')\n",
        "                columns.append(news_count + '_text')\n",
        "                columns.append(news_count + '_publish_date')\n",
        "                count += 1\n",
        "                if count >= max_count:\n",
        "                    break\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "    news_data_df = pd.DataFrame(news_data_dict, index=[0],\n",
        "                                columns=news_cols)\n",
        "\n",
        "    \"\"\"\n",
        "    if os.path.exists(output_file_name):\n",
        "        keep_header = False\n",
        "    else:\n",
        "        keep_header = True\n",
        "\n",
        "    news_data_df.to_csv(output_file_name, mode='a', header=keep_header)\n",
        "    \"\"\"\n",
        "    news_data_df.to_csv(output_file_name)\n",
        "\n",
        "    return news_data_dict\n",
        "\n",
        "\n",
        "def google_news_scrapper(start_date, end_date, output_file_name):\n",
        "    step_obj = datetime.timedelta(days=1)\n",
        "    start_date_time_obj = datetime.datetime.strptime(start_date, fmt)\n",
        "    end_date_time_obj = datetime.datetime.strptime(end_date, fmt)\n",
        "\n",
        "    while start_date_time_obj <= end_date_time_obj:\n",
        "        start_date = start_date_time_obj.strftime(fmt)\n",
        "        print(start_date)\n",
        "        run_google_news_scrapper(min_date=start_date, max_date=start_date, output_file=output_file_name)\n",
        "        time.sleep(np.random.randint(2, 5))\n",
        "        start_date_time_obj += step_obj\n",
        "\n",
        "\n",
        "def sort_news_report(input_file_name, cleaned_output_file_name, save_index=False):\n",
        "    df = pd.read_csv(input_file_name)\n",
        "    df = df.set_index('date', drop=True)\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    df = df.sort_index().drop_duplicates(keep='first')\n",
        "    df.to_csv(cleaned_output_file_name)\n",
        "    if save_index:\n",
        "        df_i = pd.DataFrame(df.index)\n",
        "        df_i.to_csv(cleaned_output_file_name[0:-4] + '_index.csv')\n",
        "\n",
        "\n",
        "def clean_news_report(input_file_name, cleaned_output_file_name, save_index=False):\n",
        "    master_df = pd.read_csv(input_file_name)\n",
        "    # get only given columns\n",
        "    master_df = master_df[['date', 'news_1_text', 'news_2_text', 'news_3_text', 'news_4_text', 'news_5_text', 'news_6_text',\n",
        "             'news_7_text', 'news_8_text', 'news_9_text']]\n",
        "    master_df = master_df.set_index('date', drop=True)\n",
        "    master_df.index = pd.to_datetime(master_df.index, format=fmt)\n",
        "    # soft and drop duplicates\n",
        "    master_df = master_df.sort_index().drop_duplicates(keep='first')\n",
        "    idx = np.unique(master_df.index, return_index=True)[1]\n",
        "    master_df = master_df.iloc[idx]\n",
        "    master_df.to_csv(cleaned_output_file_name)\n",
        "\n",
        "    master_df.to_csv(cleaned_output_file_name)\n",
        "    if save_index:\n",
        "        df_i = pd.DataFrame(master_df.index)\n",
        "        df_i.to_csv(cleaned_output_file_name[0:-4] + '_index.csv')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_date = '01/01/2018'\n",
        "    # end_date = datetime.datetime.now().strftime(fmt)\n",
        "    end_date = '01/02/2019'\n",
        "    news_raw_filename = 'google_news_final.csv'\n",
        "\n",
        "    google_news_scrapper(start_date, end_date, news_raw_filename)\n",
        "    news_cleaned_filename = news_raw_filename[0:-4] + '_cleaned.csv'\n",
        "    clean_news_report(news_raw_filename, news_cleaned_filename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "# from newspaper import Article\n",
        "# import datetime\n",
        "# import pandas as pd\n",
        "# import time\n",
        "# import os\n",
        "# import numpy as np pip install these\n",
        "\n",
        "!pip install requests beautifulsoup4 newspaper3k datetime pandas\n"
      ],
      "metadata": {
        "id": "6D73TZCDq6LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install   os numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6hjtVzFtOZk",
        "outputId": "2ca22682-8394-4521-fa6c-87004c6db2e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 newspaper3k datetime pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xojMGM0rNlO",
        "outputId": "80935b62-416d-4ba0-dc7d-a1dd85185246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting datetime\n",
            "  Downloading DateTime-5.5-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.2)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.4)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.1.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zope.interface (from datetime)\n",
            "  Downloading zope.interface-7.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from datetime) (2024.1)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (4.66.5)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.15.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface->datetime) (71.0.4)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DateTime-5.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zope.interface-7.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (254 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=c6056ad3e5dc209a3315ee60d4ef437b2de58734ed623aeff10566d059bd3c2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3342 sha256=2bd622b0614d3158b2f45e5dae1475a4c1eaf89a77df5ad7a7b0a6c4d620be8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=ea4f32e60a499f562cfb1dafddbd0899829e459feb4d9faacaf985aaf747ca7b\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=4f497c18917fb74adc242d989db54f083226e8a18c9e5c9883b33489db0f9377\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, zope.interface, feedparser, cssselect, requests-file, feedfinder2, datetime, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.2.0 datetime-5.5 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-2.1.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.1.2 zope.interface-7.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from newspaper import Article\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "URL =\"https://www.google.com/search?sca_esv=da0d154385835d98&sca_upv=1&sxsrf=ADLYWIJk7V2c-QQLZM2GbbKh6qYNnS6Vwg:1725179162702&q=nifty+50&tbm=nws&source=lnms&fbs=AEQNm0DkZLeKUdEdIhsqc4Dlu8nOjxyyLwFSABJ9DTHfm7z_B0NiOiOaZIPBaJ8kODmB6Tch29ybjvvWj4cFzijZ6yTSZyHjSI4wrduwP8Ic98K5qzD32CgypgWdp5Ls6bG3AEeC4zaSMzwcWViueDSiYlVwKzhl-SQHWkXG2pE6Q4ZldvYzVP7yyAaNSQMhqOQ-RFvZ_-qa3mrm4z-wLDKcfVWrwFyiuw&sa=X&ved=2ahUKEwjOsqqZqaGIAxUW1zgGHTFmD_kQ0pQJegQIDxAB&biw=1904&bih=1040&dpr=1\"\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.80 Safari/537.36',\n",
        "    'Content-Type': 'text/html',\n",
        "}\n",
        "\n",
        "max_count = 10  # max 10 news articles per day\n",
        "fmt = '%m/%d/%Y'\n",
        "news_cols = ['index', 'date', 'status_code', 'url', 'news_1_url', 'news_1_text',\n",
        "             'news_1_publish_date', 'news_2_url', 'news_2_text', 'news_2_publish_date',\n",
        "             'news_3_url', 'news_3_text', 'news_3_publish_date', 'news_4_url',\n",
        "             'news_4_text', 'news_4_publish_date', 'news_5_url', 'news_5_text',\n",
        "             'news_5_publish_date', 'news_6_url', 'news_6_text', 'news_6_publish_date',\n",
        "             'news_7_url', 'news_7_text', 'news_7_publish_date', 'news_8_url',\n",
        "             'news_8_text', 'news_8_publish_date', 'news_9_url', 'news_9_text',\n",
        "             'news_9_publish_date']\n",
        "\n",
        "\n",
        "def run_google_news_scrapper(**params):\n",
        "    output_file_name = ''\n",
        "    for key, value in params.items():\n",
        "        if key == 'min_date':\n",
        "            min_date = value\n",
        "        if key == 'output_file':\n",
        "            output_file_name = value\n",
        "\n",
        "    news_data_dict = dict()\n",
        "    columns = []\n",
        "    news_data_dict['date'] = min_date\n",
        "    columns.append('date')\n",
        "    response = requests.get(URL.format(**params), headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    news_data_dict['status_code'] = response.status_code\n",
        "    columns.append('status_code')\n",
        "    if response.status_code != 200:\n",
        "        print(\"******** fail ********** \")\n",
        "        return\n",
        "    # print(response.url)\n",
        "    news_data_dict['url'] = response.url\n",
        "    columns.append('url')\n",
        "    count = 1\n",
        "    for link in soup.find_all('a'):\n",
        "        link_str = str(link.get('href'))\n",
        "        try:\n",
        "            if link_str.startswith(\"https://\") and link_str.find('google.com') == -1 and link_str.find(\n",
        "                    \"https://www.youtube.com/\") == -1 and link_str.find(\"https://www.blogger.com/\") == -1:\n",
        "                article = Article(link_str)\n",
        "                article.download()\n",
        "                article.parse()\n",
        "                print(link_str)\n",
        "                print(article.authors)\n",
        "\n",
        "\n",
        "                news_count = 'news_' + str(count)\n",
        "                news_data_dict[news_count + '_url'] = link_str\n",
        "                news_data_dict[news_count + '_text'] = article.text\n",
        "                news_data_dict[news_count + '_publish_date'] = article.publish_date\n",
        "\n",
        "                columns.append(news_count + '_url')\n",
        "                columns.append(news_count + '_text')\n",
        "                columns.append(news_count + '_publish_date')\n",
        "                count += 1\n",
        "                if count >= max_count:\n",
        "                    break\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "    news_data_df = pd.DataFrame(news_data_dict, index=[0],\n",
        "                                columns=news_cols)\n",
        "\n",
        "\n",
        "    if os.path.exists(output_file_name):\n",
        "        keep_header = False\n",
        "    else:\n",
        "        keep_header = True\n",
        "\n",
        "    news_data_df.to_csv(output_file_name, mode='a', header=keep_header)\n",
        "\n",
        "    news_data_df.to_csv(output_file_name)\n",
        "\n",
        "    return news_data_dict\n",
        "\n",
        "\n",
        "def google_news_scrapper(start_date, end_date, output_file_name):\n",
        "    step_obj = datetime.timedelta(days=1)\n",
        "    start_date_time_obj = datetime.datetime.strptime(start_date, fmt)\n",
        "    end_date_time_obj = datetime.datetime.strptime(end_date, fmt)\n",
        "\n",
        "    while start_date_time_obj <= end_date_time_obj:\n",
        "        start_date = start_date_time_obj.strftime(fmt)\n",
        "        print(start_date)\n",
        "        run_google_news_scrapper(min_date=start_date, max_date=start_date, output_file=output_file_name)\n",
        "        time.sleep(np.random.randint(2, 5))\n",
        "        start_date_time_obj += step_obj\n",
        "\n",
        "\n",
        "def sort_news_report(input_file_name, cleaned_output_file_name, save_index=False):\n",
        "    df = pd.read_csv(input_file_name)\n",
        "    df = df.set_index('date', drop=True)\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    df = df.sort_index().drop_duplicates(keep='first')\n",
        "    df.to_csv(cleaned_output_file_name)\n",
        "    if save_index:\n",
        "        df_i = pd.DataFrame(df.index)\n",
        "        df_i.to_csv(cleaned_output_file_name[0:-4] + '_index.csv')\n",
        "\n",
        "\n",
        "def clean_news_report(input_file_name, cleaned_output_file_name, save_index=False):\n",
        "    master_df = pd.read_csv(input_file_name)\n",
        "    # get only given columns\n",
        "    master_df = master_df[['date', 'news_1_text', 'news_2_text', 'news_3_text', 'news_4_text', 'news_5_text', 'news_6_text',\n",
        "             'news_7_text', 'news_8_text', 'news_9_text']]\n",
        "    master_df = master_df.set_index('date', drop=True)\n",
        "    master_df.index = pd.to_datetime(master_df.index, format=fmt)\n",
        "    # soft and drop duplicates\n",
        "    master_df = master_df.sort_index().drop_duplicates(keep='first')\n",
        "    idx = np.unique(master_df.index, return_index=True)[1]\n",
        "    master_df = master_df.iloc[idx]\n",
        "    master_df.to_csv(cleaned_output_file_name)\n",
        "\n",
        "    master_df.to_csv(cleaned_output_file_name)\n",
        "    if save_index:\n",
        "        df_i = pd.DataFrame(master_df.index)\n",
        "        df_i.to_csv(cleaned_output_file_name[0:-4] + '_index.csv')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_date = '01/01/2024'\n",
        "\n",
        "    end_date = '01/05/2024'\n",
        "    news_raw_filename = 'google_news_final3.csv'\n",
        "\n",
        "    google_news_scrapper(start_date, end_date, news_raw_filename)\n",
        "    news_cleaned_filename = news_raw_filename[0:-4] + '_cleaned.csv'\n",
        "    clean_news_report(news_raw_filename, news_cleaned_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybMMrsqVrVP8",
        "outputId": "590cb167-c3f4-44e7-951e-0a9e90e32933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01/01/2024\n",
            "01/02/2024\n",
            "01/03/2024\n",
            "01/04/2024\n",
            "01/05/2024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pynytimes import NYTAPI\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_news(year, month, day, keyword):\n",
        "    \"\"\"\n",
        "    Gets top 10 most relevant finance news headings on each day from NY Times,\n",
        "    filtered by a given keyword.\n",
        "    \"\"\"\n",
        "    nyt = NYTAPI(\"5UI21WrJdSgZtHZpljOncwS0qMuJuOcs\", parse_dates=True)\n",
        "    list = []\n",
        "    articles = nyt.article_search(\n",
        "        results=10,\n",
        "        dates={\n",
        "            \"begin\": datetime.datetime(year, month, day),\n",
        "            \"end\": datetime.datetime(year, month, day)\n",
        "        },\n",
        "        options={\n",
        "            \"sort\": \"relevance\",\n",
        "            \"news_desk\": [\n",
        "                \"Business\", \"Business Day\", \"Entrepreneurs\", \"Financial\", \"Technology\"\n",
        "            ],\n",
        "            \"section_name\": [\n",
        "                \"Business\", \"Business Day\", \"Technology\"\n",
        "            ],\n",
        "            \"query\": keyword\n",
        "        }\n",
        "    )\n",
        "    for i in range(len(articles)):\n",
        "        list.append(articles[i]['abstract'].replace(',', \"\"))\n",
        "    return list\n",
        "\n",
        "def generate_news_file():\n",
        "    \"\"\"\n",
        "    Stores news headings everyday of Q3 2022 in csv, filtered by \"Nifty 50\".\n",
        "    \"\"\"\n",
        "    start = '2020-10-01'\n",
        "    end = '2022-09-30'\n",
        "    mydates = pd.date_range(start, end)\n",
        "    dates = []\n",
        "    for i in range(len(mydates)):\n",
        "        dates.append(mydates[i].strftime(\"%Y-%m-%d\"))\n",
        "    matrix = np.zeros((len(dates) + 1, 11), dtype=object)\n",
        "    matrix[0, 0] = \"Date\"\n",
        "\n",
        "    for i in range(10):\n",
        "        matrix[0, i + 1] = f\"News {i + 1}\"\n",
        "    for i in range(len(dates)):\n",
        "        matrix[i + 1, 0] = dates[i]\n",
        "        y, m, d = dates[i].split(\"-\")\n",
        "        news_list = get_news(int(y), int(m), int(d), \"Nifty 50\")\n",
        "        for j in range(len(news_list)):\n",
        "            matrix[i + 1, j + 1] = news_list[j]\n",
        "    df = pd.DataFrame(matrix)\n",
        "    df.to_csv(\"news_nifty50.csv\", index=False)\n",
        "\n",
        "generate_news_file()"
      ],
      "metadata": {
        "id": "Y4LVRvPRBc4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('google_news_final1.csv')"
      ],
      "metadata": {
        "id": "xsVwJNK_t7fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pynytimes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Boq5C1qsCjIa",
        "outputId": "e5328f34-4113-44a8-f4c8-36c8bdb339c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pynytimes\n",
            "  Downloading pynytimes-0.10.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from pynytimes) (2.32.3)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pynytimes) (2.0.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.10.0->pynytimes) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.10.0->pynytimes) (3.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.10.0->pynytimes) (2024.7.4)\n",
            "Downloading pynytimes-0.10.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: pynytimes\n",
            "Successfully installed pynytimes-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datetime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Oy6W26eCqEP",
        "outputId": "bf885ba4-1634-4f64-a614-92547746544f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datetime in /usr/local/lib/python3.10/dist-packages (5.5)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.10/dist-packages (from datetime) (7.0.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from datetime) (2024.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface->datetime) (71.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3IPa9oDCxNY",
        "outputId": "865e4ba0-4c8a-4a5b-ade3-e3d762566d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1O2JgU8C3qB",
        "outputId": "9e810ad8-9155-4fe8-c437-480d8ea72cbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pynytimes import NYTAPI\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_news(year, month, day, keyword):\n",
        "    \"\"\"\n",
        "    Gets top 10 most relevant finance news headings on each day from NY Times,\n",
        "    filtered by a given keyword.\n",
        "    \"\"\"\n",
        "    nyt = NYTAPI(\"K3ZFmirIyAwQgWV67ULovCQ4Ucjnv4Hx\", parse_dates=True)\n",
        "    list = []\n",
        "    articles = nyt.article_search(\n",
        "        results=10,\n",
        "        dates={\n",
        "            \"begin\": datetime.datetime(year, month, day),\n",
        "            \"end\": datetime.datetime(year, month, day)\n",
        "        },\n",
        "        options={\n",
        "            \"sort\": \"relevance\",\n",
        "            \"news_desk\": [\n",
        "                \"Business\", \"Business Day\", \"Entrepreneurs\", \"Financial\", \"Technology\"\n",
        "            ],\n",
        "            \"section_name\": [\n",
        "                \"Business\", \"Business Day\", \"Technology\"\n",
        "            ],\n",
        "            \"query\": keyword\n",
        "        }\n",
        "    )\n",
        "    for i in range(len(articles)):\n",
        "        list.append(articles[i]['abstract'].replace(',', \"\"))\n",
        "    return list\n",
        "\n",
        "def generate_news_file():\n",
        "    \"\"\"\n",
        "    Stores news headings everyday of Q3 2022 in csv, filtered by \"Nifty 50\".\n",
        "    \"\"\"\n",
        "    start = '2024-07-01'\n",
        "    end = '2024-07-03'\n",
        "    mydates = pd.date_range(start, end)\n",
        "    dates = []\n",
        "    for i in range(len(mydates)):\n",
        "        dates.append(mydates[i].strftime(\"%Y-%m-%d\"))\n",
        "    matrix = np.zeros((len(dates) + 1, 11), dtype=object)\n",
        "    matrix[0, 0] = \"Date\"\n",
        "\n",
        "    for i in range(10):\n",
        "        matrix[0, i + 1] = f\"News {i + 1}\"\n",
        "    for i in range(len(dates)):\n",
        "        matrix[i + 1, 0] = dates[i]\n",
        "        y, m, d = dates[i].split(\"-\")\n",
        "        news_list = get_news(int(y), int(m), int(d), \"Nifty 50\")\n",
        "        for j in range(len(news_list)):\n",
        "            matrix[i + 1, j + 1] = news_list[j]\n",
        "    df = pd.DataFrame(matrix)\n",
        "    df.to_csv(\"news_nifty50.csv\", index=False)\n",
        "\n",
        "generate_news_file()"
      ],
      "metadata": {
        "id": "zv4C1YM0CJ3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pynytimes import NYTAPI\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_news(year, month, day, keyword):\n",
        "    \"\"\"\n",
        "    Gets top 10 most relevant finance news headings on each day from NY Times,\n",
        "    filtered by a given keyword.\n",
        "    \"\"\"\n",
        "    nyt = NYTAPI(\"K3ZFmirIyAwQgWV67ULovCQ4Ucjnv4Hx\", parse_dates=True)\n",
        "    list = []\n",
        "    articles = nyt.article_search(\n",
        "        results=10,\n",
        "        dates={\n",
        "            \"begin\": datetime.datetime(year, month, day),\n",
        "            \"end\": datetime.datetime(year, month, day)\n",
        "        },\n",
        "        options={\n",
        "            \"sort\": \"relevance\",\n",
        "            \"news_desk\": [\n",
        "                \"Business\", \"Business Day\", \"Entrepreneurs\", \"Financial\", \"Technology\"\n",
        "            ],\n",
        "            \"section_name\": [\n",
        "                \"Business\", \"Business Day\", \"Technology\",\n",
        "            ],\n",
        "            \"query\": \"India\"\n",
        "        }\n",
        "    )\n",
        "    for i in range(len(articles)):\n",
        "        list.append(articles[i]['abstract'].replace(',', \"\"))\n",
        "    return list\n",
        "\n",
        "def generate_news_file():\n",
        "    \"\"\"\n",
        "    Stores news headings everyday of Q3 2022 in csv, filtered by \"Nifty 50\".\n",
        "    \"\"\"\n",
        "    start = '2024-07-01'\n",
        "    end = '2024-07-03'\n",
        "    mydates = pd.date_range(start, end)\n",
        "    dates = []\n",
        "    for i in range(len(mydates)):\n",
        "        dates.append(mydates[i].strftime(\"%Y-%m-%d\"))\n",
        "    matrix = np.zeros((len(dates) + 1, 11), dtype=object)\n",
        "    matrix[0, 0] = \"Date\"\n",
        "\n",
        "    for i in range(10):\n",
        "        matrix[0, i + 1] = f\"News {i + 1}\"\n",
        "    for i in range(len(dates)):\n",
        "        matrix[i + 1, 0] = dates[i]\n",
        "        y, m, d = dates[i].split(\"-\")\n",
        "        news_list = get_news(int(y), int(m), int(d), \"Nifty 50\")\n",
        "        for j in range(len(news_list)):\n",
        "            matrix[i + 1, j + 1] = news_list[j]\n",
        "    df = pd.DataFrame(matrix)\n",
        "    df.to_csv(\"news_nifty50.csv\", index=False)\n",
        "\n",
        "generate_news_file()"
      ],
      "metadata": {
        "id": "__Son6BjEKqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pynytimes import NYTAPI\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_news(year, month, day):\n",
        "    \"\"\"\n",
        "    get top 10 most relevent finance news headings on each day from NY times\n",
        "    \"\"\"\n",
        "    nyt = NYTAPI(\"K3ZFmirIyAwQgWV67ULovCQ4Ucjnv4Hx\", parse_dates=True)\n",
        "    list = []\n",
        "    articles = nyt.article_search(\n",
        "            results = 10,\n",
        "            dates = {\n",
        "                \"begin\": datetime.datetime(year, month, day),\n",
        "                \"end\": datetime.datetime(year, month, day)\n",
        "            },\n",
        "            options = {\n",
        "                \"sort\": \"relevance\",\n",
        "                \"news_desk\": [\n",
        "                    \"Business\", \"Business Day\", \"Entrepreneurs\", \"Financial\"\n",
        "                ],\n",
        "                \"section_name\" : [\n",
        "                    \"Business\", \"Business Day\"\n",
        "                ],\n",
        "            \"query\": \"India\"\n",
        "            }\n",
        "        )\n",
        "    for i in range(len(articles)):\n",
        "        list.append(articles[i]['abstract'].replace(',', \"\"))\n",
        "    return list\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "\n",
        "\n",
        "def generate_news_file():\n",
        "    \"\"\"\n",
        "    store news headings everyday of Q3 2022 in csv\n",
        "    \"\"\"\n",
        "    start = '2024-08-01'\n",
        "    end = '2024-08-30'\n",
        "    mydates = pd.date_range(start, end)\n",
        "    dates = []\n",
        "    for i in range(len(mydates)):\n",
        "        dates.append(mydates[i].strftime(\"%Y-%m-%d\"))\n",
        "    matrix = np.zeros((len(dates) + 1, 11), dtype=object)\n",
        "    matrix[0, 0] = \"Date\"\n",
        "\n",
        "    for i in range(10):\n",
        "        matrix[0, i + 1] = f\"News {i + 1}\"\n",
        "    for i in range(len(dates)):\n",
        "        matrix[i + 1, 0] = dates[i]\n",
        "        y, m, d = dates[i].split(\"-\")\n",
        "        news_list = get_news(int(y), int(m), int(d))\n",
        "        for j in range(len(news_list)):\n",
        "            matrix[i + 1, j + 1] = news_list[j]\n",
        "    df = pd.DataFrame(matrix)\n",
        "    df.to_csv(\"news3.csv\", index = False)\n",
        "\n",
        "\n",
        "generate_news_file()"
      ],
      "metadata": {
        "id": "RJP0F6fdJLFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axzRxBS4RiMX",
        "outputId": "957ca82d-67c6-4b29-f302-023e1d1782b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylXoK_1cRzWA",
        "outputId": "7447f395-6472-40cf-83f0-c9c3dc295427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datetime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIiyA5btRvAI",
        "outputId": "5436a719-35be-496c-ddf7-86e5968f7b51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datetime in /usr/local/lib/python3.10/dist-packages (5.5)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.10/dist-packages (from datetime) (7.0.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from datetime) (2024.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface->datetime) (71.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bs4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoUOAotwRo_B",
        "outputId": "9872b521-d2de-49c4-9b1b-581725c462c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.6)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import datetime  # Use datetime instead of arrow\n",
        "\n",
        "def extract_news(base_url, year_range, csv_filename=\"business_standard_news.csv\"):\n",
        "    \"\"\"\n",
        "    Extracts news articles from the Business Standard website for specified years.\n",
        "\n",
        "    Args:\n",
        "        base_url (str): Base URL of the Business Standard website's archive.\n",
        "        year_range (tuple): Tuple of integers representing the starting and ending year (inclusive).\n",
        "        csv_filename (str, optional): Filename for storing the extracted data as a CSV. Defaults to \"business_standard_news.csv\".\n",
        "\n",
        "    Returns:\n",
        "        list: List of extracted news articles in dictionary format.\n",
        "    \"\"\"\n",
        "\n",
        "    extracted_articles = []\n",
        "\n",
        "    for year in range(year_range[0], year_range[1] + 1):\n",
        "        for month in range(1, 13):\n",
        "            for day in range(1, 32):  # Handle cases with 31 days properly\n",
        "                month_str = f\"{month:02d}\"\n",
        "                day_str = f\"{day:02d}\"\n",
        "                url = f\"{base_url}?print_dd={day_str}&print_mm={month_str}&print_yy={year}\"\n",
        "\n",
        "                try:\n",
        "                    response = requests.get(url)\n",
        "                    response.raise_for_status()  # Raise exception for non-200 status codes\n",
        "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "                    for article_link in soup.select(\"ul li h2 a\"):\n",
        "                        article_url = f\"http://www.business-standard.com/{article_link['href']}\"\n",
        "\n",
        "                        try:\n",
        "                            article_response = requests.get(article_url)\n",
        "                            article_response.raise_for_status()\n",
        "                            article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n",
        "\n",
        "                            article_text = \"\"\n",
        "                            for paragraph in article_soup.select(\".p-content p\"):\n",
        "                                article_text += paragraph.text + \"\\n\"\n",
        "\n",
        "                            extracted_articles.append({\n",
        "                                \"date\": datetime.date(year, month, day),\n",
        "                                \"url\": article_url,\n",
        "                                \"text\": article_text.strip()  # Remove leading/trailing whitespace\n",
        "                            })\n",
        "\n",
        "                        except requests.exceptions.RequestException as e:\n",
        "                            print(f\"Error fetching article: {e}\")\n",
        "\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    print(f\"Error fetching list page: {e}\")\n",
        "\n",
        "    if extracted_articles:\n",
        "        df = pd.DataFrame(extracted_articles)\n",
        "        df.to_csv(csv_filename, index=False)\n",
        "        print(f\"News articles successfully saved to {csv_filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_url = \"http://www.business-standard.com/todays-paper/\"\n",
        "    year_range = (2018, 2019)  # Adjust year range as needed\n",
        "    extract_news(base_url, year_range)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SOuy_tVRRYZE",
        "outputId": "cd8ee105-3694-435d-f3bb-0226e31b3aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=01&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=02&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=03&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=04&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=05&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=06&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=07&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=08&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=09&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=10&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=11&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=12&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=13&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=14&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=15&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=16&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=17&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=18&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=19&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=20&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=21&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=22&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=23&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=24&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=25&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=26&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=27&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=28&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=29&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=30&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=31&print_mm=01&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=01&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=02&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=03&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=04&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=05&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=06&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=07&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=08&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=09&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=10&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=11&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=12&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=13&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=14&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=15&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=16&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=17&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=18&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=19&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=20&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=21&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=22&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=23&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=24&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=25&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=26&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=27&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=28&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=29&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=30&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=31&print_mm=02&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=01&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=02&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=03&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=04&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=05&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=06&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=07&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=08&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=09&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=10&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=11&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=12&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=13&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=14&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=15&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=16&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=17&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=18&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=19&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=20&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=21&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=22&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=23&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=24&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=25&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=26&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=27&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=28&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=29&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=30&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=31&print_mm=03&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=01&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=02&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=03&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=04&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=05&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=06&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=07&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=08&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=09&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=10&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=11&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=12&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=13&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=14&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=15&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=16&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=17&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=18&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=19&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=20&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=21&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=22&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=23&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=24&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=25&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=26&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=27&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=28&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=29&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=30&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=31&print_mm=04&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=01&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=02&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=03&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=04&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=05&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=06&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=07&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=08&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=09&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=10&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=11&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=12&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=13&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=14&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=15&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=16&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=17&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=18&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=19&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=20&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=21&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=22&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=23&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=24&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=25&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=26&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=27&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=28&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=29&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=30&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=31&print_mm=05&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=01&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=02&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=03&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=04&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=05&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=06&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=07&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=08&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=09&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=10&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=11&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=12&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=13&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=14&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=15&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=16&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=17&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=18&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=19&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=20&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=21&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=22&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=23&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=24&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=25&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=26&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=27&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=28&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=29&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=30&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=31&print_mm=06&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=01&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=02&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=03&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=04&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=05&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=06&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=07&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=08&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=09&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=10&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=11&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=12&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=13&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=14&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=15&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=16&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=17&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=18&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=19&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=20&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=21&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=22&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=23&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=24&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=25&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=26&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=27&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=28&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=29&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=30&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=31&print_mm=07&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=01&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=02&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=03&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=04&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=05&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=06&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=07&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=08&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=09&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=10&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=11&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=12&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=13&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=14&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=15&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=16&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=17&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=18&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=19&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=20&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=21&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=22&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=23&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=24&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=25&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=26&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=27&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=28&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=29&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=30&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=31&print_mm=08&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=01&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=02&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=03&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=04&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=05&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=06&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=07&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=08&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=09&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=10&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=11&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=12&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=13&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=14&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=15&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=16&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=17&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=18&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=19&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=20&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=21&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=22&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=23&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=24&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=25&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=26&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=27&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=28&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=29&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=30&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=31&print_mm=09&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=01&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=02&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=03&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=04&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=05&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=06&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=07&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=08&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=09&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=10&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=11&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=12&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=13&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=14&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=15&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=16&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=17&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=18&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=19&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=20&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=21&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=22&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=23&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=24&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=25&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=26&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=27&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=28&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=29&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=30&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=31&print_mm=10&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=01&print_mm=11&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=02&print_mm=11&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=03&print_mm=11&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=04&print_mm=11&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=05&print_mm=11&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=06&print_mm=11&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=07&print_mm=11&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=08&print_mm=11&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=09&print_mm=11&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=10&print_mm=11&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=11&print_mm=11&print_yy=2018\n",
            "Error fetching list page: 403 Client Error: Forbidden for url: http://www.business-standard.com/todays-paper/?print_dd=12&print_mm=11&print_yy=2018\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-0481768d7760>\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mbase_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://www.business-standard.com/todays-paper/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0myear_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2018\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2019\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust year range as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mextract_news\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-0481768d7760>\u001b[0m in \u001b[0;36mextract_news\u001b[0;34m(base_url, year_range, csv_filename)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                     \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Raise exception for non-200 status codes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;31m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             conn.request(\n\u001b[0m\u001b[1;32m    498\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# If we're given a body we start sending that in chunks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m     def request(self, method, url, body=None, headers={}, *,\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\\r\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotConnected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tunnel_host\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;31m# If we're tunneling it means we're connected to our proxy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \"\"\"\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             sock = connection.create_connection(\n\u001b[0m\u001b[1;32m    204\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "def extract_moneycontrol_news(year_range, csv_filename=\"moneycontrol_news.csv\"):\n",
        "    \"\"\"\n",
        "    Extracts news articles from Moneycontrol for specified years.\n",
        "\n",
        "    Args:\n",
        "        year_range (tuple): Tuple of integers representing the starting and ending year (inclusive).\n",
        "        csv_filename (str, optional): Filename for storing the extracted data as a CSV. Defaults to \"moneycontrol_news.csv\".\n",
        "\n",
        "    Returns:\n",
        "        list: List of extracted news articles in dictionary format.\n",
        "    \"\"\"\n",
        "\n",
        "    extracted_articles = []\n",
        "\n",
        "    for year in range(year_range[0], year_range[1] + 1):\n",
        "        for month in range(1, 13):\n",
        "            for day in range(1, 32):\n",
        "                month_str = f\"{month:02d}\"\n",
        "                day_str = f\"{day:02d}\"\n",
        "\n",
        "                # Replace with the appropriate URL structure for Moneycontrol's archive\n",
        "                url = f\"https://www.moneycontrol.com/news/archive/{year}/{month_str}/{day_str}\"\n",
        "\n",
        "                try:\n",
        "                    response = requests.get(url)\n",
        "                    response.raise_for_status()  # Raise exception for non-200 status codes\n",
        "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "                    # Adjust selectors based on Moneycontrol's HTML structure\n",
        "                    article_links = soup.select(\".clearfix.article_list li a\")\n",
        "\n",
        "                    for article_link in article_links:\n",
        "                        article_url = article_link['href']\n",
        "\n",
        "                        try:\n",
        "                            article_response = requests.get(article_url)\n",
        "                            article_response.raise_for_status()\n",
        "                            article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n",
        "\n",
        "                            # Adjust selectors based on Moneycontrol's article structure\n",
        "                            article_title = article_soup.select_one(\"h1\").text\n",
        "                            article_content = article_soup.select_one(\".article_content\").text\n",
        "\n",
        "                            extracted_articles.append({\n",
        "                                \"date\": datetime.date(year, month, day),\n",
        "                                \"url\": article_url,\n",
        "                                \"title\": article_title,\n",
        "                                \"content\": article_content.strip()\n",
        "                            })\n",
        "\n",
        "                        except requests.exceptions.RequestException as e:\n",
        "                            print(f\"Error fetching article: {e}\")\n",
        "\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    print(f\"Error fetching list page: {e}\")\n",
        "\n",
        "    if extracted_articles:\n",
        "        df = pd.DataFrame(extracted_articles)\n",
        "        df.to_csv(csv_filename, index=False)\n",
        "        print(f\"News articles successfully saved to {csv_filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    year_range = (2023, 2023)  # Adjust year range as needed\n",
        "    extract_moneycontrol_news(year_range)"
      ],
      "metadata": {
        "id": "AOyeXq75TV21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVrhA3PMADGP",
        "outputId": "11719be1-0858-4dfb-8a63-a3d75685e383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "huggingface-cli login\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "uNScLkO4AJjB",
        "outputId": "eee51195-c0fb-4184-da87-d2e2edd75f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-7-f67f8e96d4cc>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-f67f8e96d4cc>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    huggingface-cli login\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "def finbert_sentiment_score(heading):\n",
        "    \"\"\"\n",
        "    Compute sentiment score using pretrained FinBERT on -1 to 1 scale.\n",
        "    -1 being negative and 1 being positive\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
        "    model = AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
        "\n",
        "    inputs = tokenizer(heading, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Assuming binary classification (positive or negative)\n",
        "    predicted_class_index = logits.argmax(-1).item()\n",
        "    predicted_class = model.config.id2label[predicted_class_index]\n",
        "\n",
        "    # Calculate sentiment score based on predicted class\n",
        "    if predicted_class == \"positive\":\n",
        "        return 1.0\n",
        "    elif predicted_class == \"negative\":\n",
        "        return -1.0\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "news_df = pd.read_csv(\"merged_data1.csv\")\n",
        "\n",
        "finbert_sentiment = []\n",
        "for i in range(len(news_df)):\n",
        "    # Combine title and news list into a sentence\n",
        "    heading = news_df.iloc[i]['title'] + ' ' + ' '.join(news_df.iloc[i, 1:].tolist())\n",
        "\n",
        "    # Remove '0' values from the news list\n",
        "    news_list = [item for item in news_df.iloc[i, 1:].tolist() if item != '0']\n",
        "\n",
        "    # Check if heading is empty or contains only whitespace\n",
        "    if heading.strip() == '':\n",
        "        score_finbert = 0\n",
        "    else:\n",
        "        score_finbert = finbert_sentiment_score(heading)\n",
        "\n",
        "    finbert_sentiment.append(score_finbert)\n",
        "\n",
        "news_df['FinBERT score'] = finbert_sentiment\n",
        "\n",
        "news_df.to_csv(\"sentiment3.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "-TQJaou8-CSg",
        "outputId": "ca2313ce-dd45-4d81-a1be-fc319b15dd5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3548b3b97755>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mscore_finbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mscore_finbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinbert_sentiment_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheading\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mfinbert_sentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_finbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-3548b3b97755>\u001b[0m in \u001b[0;36mfinbert_sentiment_score\u001b[0;34m(heading)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \"\"\"\n\u001b[1;32m      9\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ProsusAI/finbert'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ProsusAI/finbert'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheading\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sharded\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3637\u001b[0m                 \u001b[0;31m# Time to load the checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3638\u001b[0;31m                 \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3640\u001b[0m             \u001b[0;31m# set dtype to instantiate the model under:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"mmap\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mweights_only_kwarg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"weights_only\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_greater_or_equal_than_1_13\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         return torch.load(\n\u001b[0m\u001b[1;32m    543\u001b[0m             \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_weights_only_unpickler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0mtyped_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m             typed_storage._untyped_storage._set_from_file(\n\u001b[0m\u001b[1;32m   1358\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m                 torch._utils._element_size(typed_storage.dtype))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def FinBERT_sentiment_score(heading):\n",
        "    \"\"\"\n",
        "    compute sentiment score using pretrained FinBERT on -1 to 1 scale. -1 being negative and 1 being positive\n",
        "    \"\"\"\n",
        "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "    from transformers import pipeline\n",
        "    tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
        "    finbert = AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
        "    nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n",
        "    result = nlp(heading)\n",
        "    if result[0]['label'] == \"positive\":\n",
        "        return result[0]['score']\n",
        "    elif result[0]['label'] == \"neutral\":\n",
        "        return 0\n",
        "    else:\n",
        "        return (0 - result[0]['score'])\n",
        "\n",
        "\n",
        "def VADER_sentiment_score(heading):\n",
        "    \"\"\"\n",
        "    compute sentiment score using pretrained VADER on -1 to 1 scale. -1 being negative and 1 being positive\n",
        "    \"\"\"\n",
        "    import nltk\n",
        "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "    nltk.download('vader_lexicon')\n",
        "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    result = analyzer.polarity_scores(heading)\n",
        "    if result['pos'] == max(result['neg'], result['neu'], result['pos']):\n",
        "        return result['pos']\n",
        "    if result['neg'] == max(result['neg'], result['neu'], result['pos']):\n",
        "        return (0 - result['neg'])\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "news_df = pd.read_csv(\"1August.csv\")\n",
        "\n",
        "\n",
        "\n",
        "BERT_sentiment = []\n",
        "\n",
        "\n",
        "for i in range(len(news_df)):\n",
        "    news_list = news_df.iloc[i, 1:].tolist()\n",
        "    news_list = [i for i in news_list if i != '0']\n",
        "    score_BERT = FinBERT_sentiment_score(news_list)\n",
        "    BERT_sentiment.append(score_BERT)\n",
        "\n",
        "\n",
        "# print(news_df.iloc[129])\n",
        "\n",
        "news_df['FinBERT score'] = BERT_sentiment\n",
        "\n",
        "news_df.to_csv(\"sentiment.csv\")"
      ],
      "metadata": {
        "id": "e6zQ37eY-8Xg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
        "finbert = AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
        "nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n",
        "\n",
        "def FinBERT_sentiment_score(headings):\n",
        "    \"\"\"\n",
        "    Compute sentiment score using pretrained FinBERT on -1 to 1 scale for a batch of headings.\n",
        "    -1 being negative and 1 being positive\n",
        "    \"\"\"\n",
        "    results = nlp(headings)\n",
        "    scores = []\n",
        "    for result in results:\n",
        "        if result['label'] == \"positive\":\n",
        "            scores.append(result['score'])\n",
        "        elif result['label'] == \"neutral\":\n",
        "            scores.append(0)\n",
        "        else:\n",
        "            scores.append((0 - result['score']))\n",
        "    return scores\n",
        "\n",
        "news_df = pd.read_csv(\"30August.csv\")\n",
        "\n",
        "batch_size = 100  # You can adjust this based on your memory limitations\n",
        "finbert_sentiment = []\n",
        "for i in range(0, len(news_df), batch_size):\n",
        "    batch_headings = []\n",
        "    for j in range(i, min(i + batch_size, len(news_df))):\n",
        "        news_list = news_df.iloc[j, 1:].tolist()\n",
        "        news_list = [i for i in news_list if i != '0']\n",
        "        heading = ' '.join(news_list)\n",
        "        batch_headings.append(heading)\n",
        "    scores = FinBERT_sentiment_score(batch_headings)\n",
        "    finbert_sentiment.extend(scores)\n",
        "\n",
        "news_df['FinBERT score'] = finbert_sentiment\n",
        "\n",
        "news_df.to_csv(\"sentiment35.csv\")"
      ],
      "metadata": {
        "id": "udsu59JtCd3x"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}